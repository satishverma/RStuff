
\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{hyperref,authblk}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\title{Notes on the Buzz data}
\author{Nina Zumel}
\author{John Mount}
\affil{Win-Vector LLC}
\date{11-6-2013}
\maketitle

To run this example you need a system with R installed 
(see \url{http://cran.r-project.org}),
Latex (see \url{http://tug.org}) and data from 
\url{https://github.com/WinVector/zmPDSwR/tree/master/Buzz}.

We are not perfoming any new analysis here, just supplying a direct application of Random Forests on the data.

Data from: \url{http://ama.liglab.fr/datasets/buzz/}
Using: 
   \url{http://ama.liglab.fr/datasets/buzz/classification/TomsHardware/Relative_labeling/sigma=500/TomsHardware-Relative-Sigma-500.data}

(described in \url{http://ama.liglab.fr/datasets/buzz/classification/TomsHardware/Relative_labeling/sigma=500/TomsHardware-Relative-Sigma-500.names} )

\begin{verbatim}
Crypto hashes:
$ shasum TomsHardware-*.txt
  5a1cc7863a9da8d6e8380e1446f25eec2032bd91  TomsHardware-Absolute-Sigma-500.data.txt
  86f2c0f4fba4fb42fe4ee45b48078ab51dba227e  TomsHardware-Absolute-Sigma-500.names.txt
  c239182c786baf678b55f559b3d0223da91e869c  TomsHardware-Relative-Sigma-500.data.txt
  ec890723f91ae1dc87371e32943517bcfcd9e16a  TomsHardware-Relative-Sigma-500.names.txt
\end{verbatim}

To run this example:
\begin{enumerate}
\item Download buzz.Rnw and TomsHardware-Relative-Sigma-500.data.txt from the GitHub URL.
\item Start a copy of R, use {\tt setwd()} to move to the directory you have stored the files.
\item Make sure {\tt knitr} is loaded into R ( {\tt install.packages('knitr')} and
{\tt library(knitr)} ).
\item In R run: (produces buzz.tex from buzz.Rnw).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{knit}\hlstd{(}\hlstr{'buzz.Rnw'}\hlstd{)}
\hlkwd{system}\hlstd{(}\hlstr{'/usr/texbin/pdflatex buzz.tex'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{enumerate}

% set up caching and knitr chunk dependency calculation
% note: you will want to do clean re-runs once in a while to make sure 
% you are not seeing stale cache results.



Now you can run the following data prep steps:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{infile} \hlkwb{<-} \hlstr{"TomsHardware-Relative-Sigma-500.data.txt"}
\hlkwd{paste}\hlstd{(}\hlstr{'checked at'}\hlstd{,}\hlkwd{date}\hlstd{())}
\end{alltt}
\begin{verbatim}
## [1] "checked at Fri Nov  8 15:01:39 2013"
\end{verbatim}
\begin{alltt}
\hlkwd{system}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'shasum'}\hlstd{,infile),}\hlkwc{intern}\hlstd{=T)}  \hlcom{# write down file hash}
\end{alltt}
\begin{verbatim}
## [1] "c239182c786baf678b55f559b3d0223da91e869c  TomsHardware-Relative-Sigma-500.data.txt"
\end{verbatim}
\begin{alltt}
\hlstd{buzzdata} \hlkwb{<-} \hlkwd{read.table}\hlstd{(infile,} \hlkwc{header}\hlstd{=F,} \hlkwc{sep}\hlstd{=}\hlstr{","}\hlstd{)}
\hlstd{makevars} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{colname}\hlstd{,} \hlkwc{ndays}\hlstd{=}\hlnum{7}\hlstd{) \{}
  \hlkwd{paste}\hlstd{(colname,} \hlnum{0}\hlopt{:}\hlstd{ndays,} \hlkwc{sep}\hlstd{=}\hlstr{''}\hlstd{)}
\hlstd{\}}
\hlstd{varnames} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"num.new.disc"}\hlstd{,}
             \hlstr{"burstiness"}\hlstd{,}
             \hlstr{"number.total.disc"}\hlstd{,}
             \hlstr{"auth.increase"}\hlstd{,}
             \hlstr{"atomic.containers"}\hlstd{,} \hlcom{# not documented}
             \hlstr{"num.displays"}\hlstd{,} \hlcom{# number of times topic displayed to user (measure of interest)}
             \hlstr{"contribution.sparseness"}\hlstd{,} \hlcom{# not documented}
             \hlstr{"avg.auths.per.disc"}\hlstd{,}
             \hlstr{"num.authors.topic"}\hlstd{,} \hlcom{# total authors on the topic}
             \hlstr{"avg.disc.length"}\hlstd{,}
             \hlstr{"attention.level.author"}\hlstd{,}
             \hlstr{"attention.level.contrib"}
\hlstd{)}
\hlstd{colnames} \hlkwb{<-} \hlkwd{as.vector}\hlstd{(}\hlkwd{sapply}\hlstd{(varnames,} \hlkwc{FUN}\hlstd{=makevars))}
\hlstd{colnames} \hlkwb{<-}  \hlkwd{c}\hlstd{(colnames,} \hlstr{"buzz"}\hlstd{)}
\hlkwd{colnames}\hlstd{(buzzdata)} \hlkwb{<-} \hlstd{colnames}
\hlcom{# Split into training and test}
\hlkwd{set.seed}\hlstd{(}\hlnum{2362690L}\hlstd{)}
\hlstd{rgroup} \hlkwb{<-} \hlkwd{runif}\hlstd{(}\hlkwd{dim}\hlstd{(buzzdata)[}\hlnum{1}\hlstd{])}
\hlstd{buzztrain} \hlkwb{<-} \hlstd{buzzdata[rgroup} \hlopt{>} \hlnum{0.1}\hlstd{,]}
\hlstd{buzztest} \hlkwb{<-} \hlstd{buzzdata[rgroup} \hlopt{<=}\hlnum{0.1}\hlstd{,]}
\end{alltt}
\end{kframe}
\end{knitrout}


This currently returns a training set with 7114 rows and a test set with 
791 rows, which 
is the same
as when this document was prepared.

Notice we have exploded the basic column names into the following:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(colnames)}
\end{alltt}
\begin{verbatim}
##  [1] "num.new.disc0"            "num.new.disc1"           
##  [3] "num.new.disc2"            "num.new.disc3"           
##  [5] "num.new.disc4"            "num.new.disc5"           
##  [7] "num.new.disc6"            "num.new.disc7"           
##  [9] "burstiness0"              "burstiness1"             
## [11] "burstiness2"              "burstiness3"             
## [13] "burstiness4"              "burstiness5"             
## [15] "burstiness6"              "burstiness7"             
## [17] "number.total.disc0"       "number.total.disc1"      
## [19] "number.total.disc2"       "number.total.disc3"      
## [21] "number.total.disc4"       "number.total.disc5"      
## [23] "number.total.disc6"       "number.total.disc7"      
## [25] "auth.increase0"           "auth.increase1"          
## [27] "auth.increase2"           "auth.increase3"          
## [29] "auth.increase4"           "auth.increase5"          
## [31] "auth.increase6"           "auth.increase7"          
## [33] "atomic.containers0"       "atomic.containers1"      
## [35] "atomic.containers2"       "atomic.containers3"      
## [37] "atomic.containers4"       "atomic.containers5"      
## [39] "atomic.containers6"       "atomic.containers7"      
## [41] "num.displays0"            "num.displays1"           
## [43] "num.displays2"            "num.displays3"           
## [45] "num.displays4"            "num.displays5"           
## [47] "num.displays6"            "num.displays7"           
## [49] "contribution.sparseness0" "contribution.sparseness1"
## [51] "contribution.sparseness2" "contribution.sparseness3"
## [53] "contribution.sparseness4" "contribution.sparseness5"
## [55] "contribution.sparseness6" "contribution.sparseness7"
## [57] "avg.auths.per.disc0"      "avg.auths.per.disc1"     
## [59] "avg.auths.per.disc2"      "avg.auths.per.disc3"     
## [61] "avg.auths.per.disc4"      "avg.auths.per.disc5"     
## [63] "avg.auths.per.disc6"      "avg.auths.per.disc7"     
## [65] "num.authors.topic0"       "num.authors.topic1"      
## [67] "num.authors.topic2"       "num.authors.topic3"      
## [69] "num.authors.topic4"       "num.authors.topic5"      
## [71] "num.authors.topic6"       "num.authors.topic7"      
## [73] "avg.disc.length0"         "avg.disc.length1"        
## [75] "avg.disc.length2"         "avg.disc.length3"        
## [77] "avg.disc.length4"         "avg.disc.length5"        
## [79] "avg.disc.length6"         "avg.disc.length7"        
## [81] "attention.level.author0"  "attention.level.author1" 
## [83] "attention.level.author2"  "attention.level.author3" 
## [85] "attention.level.author4"  "attention.level.author5" 
## [87] "attention.level.author6"  "attention.level.author7" 
## [89] "attention.level.contrib0" "attention.level.contrib1"
## [91] "attention.level.contrib2" "attention.level.contrib3"
## [93] "attention.level.contrib4" "attention.level.contrib5"
## [95] "attention.level.contrib6" "attention.level.contrib7"
## [97] "buzz"
\end{verbatim}
\end{kframe}
\end{knitrout}


We are now ready to create a simple model predicting ``buzz'' as function of the
other columns.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# build a model}
\hlcom{# let's use all the input variables}
\hlstd{nlist} \hlkwb{=} \hlstd{varnames}
\hlstd{varslist} \hlkwb{=} \hlkwd{as.vector}\hlstd{(}\hlkwd{sapply}\hlstd{(nlist,} \hlkwc{FUN}\hlstd{=makevars))}
\hlcom{# these were defined previously,  in Chapter 9}
\hlstd{loglikelihood} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{,} \hlkwc{py}\hlstd{) \{}
  \hlstd{pysmooth} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(py}\hlopt{==}\hlnum{0}\hlstd{,} \hlnum{1e-12}\hlstd{,}
                     \hlkwd{ifelse}\hlstd{(py}\hlopt{==}\hlnum{1}\hlstd{,} \hlnum{1}\hlopt{-}\hlnum{1e-12}\hlstd{, py))}
  \hlkwd{sum}\hlstd{(y} \hlopt{*} \hlkwd{log}\hlstd{(pysmooth)} \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{y)}\hlopt{*}\hlkwd{log}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{pysmooth))}
\hlstd{\}}
\hlstd{accuracyMeasures} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{pred}\hlstd{,} \hlkwc{truth}\hlstd{,} \hlkwc{threshold}\hlstd{=}\hlnum{0.5}\hlstd{,} \hlkwc{name}\hlstd{=}\hlstr{"model"}\hlstd{) \{}
  \hlstd{dev.norm} \hlkwb{<-} \hlopt{-}\hlnum{2}\hlopt{*}\hlkwd{loglikelihood}\hlstd{(}\hlkwd{as.numeric}\hlstd{(truth), pred)}\hlopt{/}\hlkwd{length}\hlstd{(pred)}
  \hlstd{ctable} \hlkwb{=} \hlkwd{table}\hlstd{(}\hlkwc{truth}\hlstd{=truth,}
                 \hlkwc{pred}\hlstd{=pred)}
  \hlstd{accuracy} \hlkwb{<-} \hlkwd{sum}\hlstd{(}\hlkwd{diag}\hlstd{(ctable))}\hlopt{/}\hlkwd{sum}\hlstd{(ctable)}
  \hlstd{precision} \hlkwb{<-} \hlstd{ctable[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]}\hlopt{/}\hlkwd{sum}\hlstd{(ctable[,}\hlnum{2}\hlstd{])}
  \hlstd{recall} \hlkwb{<-} \hlstd{ctable[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]}\hlopt{/}\hlkwd{sum}\hlstd{(ctable[}\hlnum{2}\hlstd{,])}
  \hlstd{f1} \hlkwb{<-} \hlstd{precision}\hlopt{*}\hlstd{recall}
  \hlkwd{print}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{"precision="}\hlstd{, precision,} \hlstr{"; recall="} \hlstd{, recall))}
  \hlkwd{print}\hlstd{(ctable)}
  \hlkwd{data.frame}\hlstd{(}\hlkwc{model}\hlstd{=name,} \hlkwc{accuracy}\hlstd{=accuracy,} \hlkwc{f1}\hlstd{=f1, dev.norm)}
\hlstd{\}}
\hlkwd{library}\hlstd{(randomForest)}
\hlstd{bzFormula} \hlkwb{<-} \hlkwd{paste}\hlstd{(}\hlstr{'as.factor(buzz) ~ '}\hlstd{,}\hlkwd{paste}\hlstd{(varslist,}\hlkwc{collapse}\hlstd{=}\hlstr{' + '}\hlstd{))}
\hlstd{fmodel} \hlkwb{<-} \hlkwd{randomForest}\hlstd{(}\hlkwd{as.formula}\hlstd{(bzFormula),}
                      \hlkwc{data}\hlstd{=buzztrain,}
                      \hlkwc{mtry}\hlstd{=}\hlkwd{floor}\hlstd{(}\hlkwd{sqrt}\hlstd{(}\hlkwd{length}\hlstd{(varslist))),}
                      \hlkwc{ntree}\hlstd{=}\hlnum{101}\hlstd{,}
                      \hlkwc{importance}\hlstd{=T)}
\hlkwd{print}\hlstd{(}\hlstr{'training'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "training"
\end{verbatim}
\begin{alltt}
\hlstd{rtrain} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{truth}\hlstd{=buzztrain}\hlopt{$}\hlstd{buzz,} \hlkwc{pred}\hlstd{=}\hlkwd{predict}\hlstd{(fmodel,} \hlkwc{newdata}\hlstd{=buzztrain))}
\hlkwd{print}\hlstd{(}\hlkwd{accuracyMeasures}\hlstd{(rtrain}\hlopt{$}\hlstd{pred, rtrain}\hlopt{$}\hlstd{truth))}
\end{alltt}
\begin{verbatim}
## [1] "precision= 1 ; recall= 0.999360613810742"
##      pred
## truth    0    1
##     0 5550    0
##     1    1 1563
##   model accuracy     f1 dev.norm
## 1 model   0.9999 0.9994 0.007768
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlstr{'test'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "test"
\end{verbatim}
\begin{alltt}
\hlstd{rtest} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{truth}\hlstd{=buzztest}\hlopt{$}\hlstd{buzz,} \hlkwc{pred}\hlstd{=}\hlkwd{predict}\hlstd{(fmodel,} \hlkwc{newdata}\hlstd{=buzztest))}
\hlkwd{print}\hlstd{(}\hlkwd{accuracyMeasures}\hlstd{(rtest}\hlopt{$}\hlstd{pred, rtest}\hlopt{$}\hlstd{truth))}
\end{alltt}
\begin{verbatim}
## [1] "precision= 0.831460674157303 ; recall= 0.836158192090395"
##      pred
## truth   0   1
##     0 584  30
##     1  29 148
##   model accuracy     f1 dev.norm
## 1 model   0.9254 0.6952    4.122
\end{verbatim}
\end{kframe}
\end{knitrout}


Notice the extreme fall-off from training to test performance, the random forest
over fit.  In fact the random forest fit all the data if it sees it during training:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fmodel} \hlkwb{<-} \hlkwd{randomForest}\hlstd{(}\hlkwd{as.formula}\hlstd{(bzFormula),}
                      \hlkwc{data}\hlstd{=buzzdata,}
                      \hlkwc{mtry}\hlstd{=}\hlkwd{floor}\hlstd{(}\hlkwd{sqrt}\hlstd{(}\hlkwd{length}\hlstd{(varslist))),}
                      \hlkwc{ntree}\hlstd{=}\hlnum{101}\hlstd{,}
                      \hlkwc{importance}\hlstd{=T)}
\hlkwd{print}\hlstd{(}\hlstr{'all data'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "all data"
\end{verbatim}
\begin{alltt}
\hlstd{rall} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{truth}\hlstd{=buzztrain}\hlopt{$}\hlstd{buzz,} \hlkwc{pred}\hlstd{=}\hlkwd{predict}\hlstd{(fmodel,} \hlkwc{newdata}\hlstd{=buzztrain))}
\hlkwd{print}\hlstd{(}\hlkwd{accuracyMeasures}\hlstd{(rall}\hlopt{$}\hlstd{pred, rall}\hlopt{$}\hlstd{truth))}
\end{alltt}
\begin{verbatim}
## [1] "precision= 1 ; recall= 0.999360613810742"
##      pred
## truth    0    1
##     0 5550    0
##     1    1 1563
##   model accuracy     f1 dev.norm
## 1 model   0.9999 0.9994 0.007768
\end{verbatim}
\end{kframe}
\end{knitrout}


To try and control the over-fitting we build a new model with the tree
complexity limited to 100 nodes and the node size to at least 20.
This is not necessarily a better model (in fact it scores slightly
poorer on test), but it is one where the training procedure didn't
have enough freedom to memorize the training data (and therefore maybe
had visibility into some trade-offs.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fmodel} \hlkwb{<-} \hlkwd{randomForest}\hlstd{(}\hlkwd{as.formula}\hlstd{(bzFormula),}
                      \hlkwc{data}\hlstd{=buzztrain,}
                      \hlkwc{mtry}\hlstd{=}\hlkwd{floor}\hlstd{(}\hlkwd{sqrt}\hlstd{(}\hlkwd{length}\hlstd{(varslist))),}
                      \hlkwc{ntree}\hlstd{=}\hlnum{101}\hlstd{,}
                      \hlkwc{maxnodes}\hlstd{=}\hlnum{100}\hlstd{,}
                      \hlkwc{nodesize}\hlstd{=}\hlnum{20}\hlstd{,}
                      \hlkwc{importance}\hlstd{=T)}
\hlkwd{print}\hlstd{(}\hlstr{'training'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "training"
\end{verbatim}
\begin{alltt}
\hlstd{rtrain} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{truth}\hlstd{=buzztrain}\hlopt{$}\hlstd{buzz,} \hlkwc{pred}\hlstd{=}\hlkwd{predict}\hlstd{(fmodel,} \hlkwc{newdata}\hlstd{=buzztrain))}
\hlkwd{print}\hlstd{(}\hlkwd{accuracyMeasures}\hlstd{(rtrain}\hlopt{$}\hlstd{pred, rtrain}\hlopt{$}\hlstd{truth))}
\end{alltt}
\begin{verbatim}
## [1] "precision= 0.864364981504316 ; recall= 0.896419437340154"
##      pred
## truth    0    1
##     0 5330  220
##     1  162 1402
##   model accuracy     f1 dev.norm
## 1 model   0.9463 0.7748    2.967
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlstr{'test'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] "test"
\end{verbatim}
\begin{alltt}
\hlstd{rtest} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{truth}\hlstd{=buzztest}\hlopt{$}\hlstd{buzz,} \hlkwc{pred}\hlstd{=}\hlkwd{predict}\hlstd{(fmodel,} \hlkwc{newdata}\hlstd{=buzztest))}
\hlkwd{print}\hlstd{(}\hlkwd{accuracyMeasures}\hlstd{(rtest}\hlopt{$}\hlstd{pred, rtest}\hlopt{$}\hlstd{truth))}
\end{alltt}
\begin{verbatim}
## [1] "precision= 0.809782608695652 ; recall= 0.84180790960452"
##      pred
## truth   0   1
##     0 579  35
##     1  28 149
##   model accuracy     f1 dev.norm
## 1 model   0.9204 0.6817    4.401
\end{verbatim}
\end{kframe}
\end{knitrout}


And we can also make plots.

Training performance:
% Trick: since this block is cached the side-effect (saving a new copy
% of the PDF will not happen unless the block is re-run.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(ggplot2)}
\hlkwd{ggplot}\hlstd{(rtrain,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=pred,} \hlkwc{color}\hlstd{=(truth}\hlopt{==}\hlnum{1}\hlstd{),}\hlkwc{linetype}\hlstd{=(truth}\hlopt{==}\hlnum{1}\hlstd{)))} \hlopt{+}
   \hlkwd{geom_density}\hlstd{(}\hlkwc{adjust}\hlstd{=}\hlnum{0.1}\hlstd{,)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/plottrain} 

\end{knitrout}


Test performance:
% Trick: since this block is cached the side-effect (saving a new copy
% of the PDF will not happen unless the block is re-run.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(rtest,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=pred,} \hlkwc{color}\hlstd{=(truth}\hlopt{==}\hlnum{1}\hlstd{),}\hlkwc{linetype}\hlstd{=(truth}\hlopt{==}\hlnum{1}\hlstd{)))} \hlopt{+}
   \hlkwd{geom_density}\hlstd{(}\hlkwc{adjust}\hlstd{=}\hlnum{0.1}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/plottest} 

\end{knitrout}


Note the classifier scores are concentrated near zero and one
(meaning the printed confusion matrices pretty much capture the whole
story and the density plots or any sort of ROC plot doesn't add much
value in this case).

\pagebreak

Save prepared R environment.
% Another way to conditionally save, check for file.
% message=F is letting message() calls get routed to console instead
% of the document.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fname} \hlkwb{<-} \hlstr{'thRS500.Rdata'}
\hlkwa{if}\hlstd{(}\hlopt{!}\hlkwd{file.exists}\hlstd{(fname)) \{}
   \hlkwd{save}\hlstd{(}\hlkwc{list}\hlstd{=}\hlkwd{ls}\hlstd{(),}\hlkwc{file}\hlstd{=fname)}
   \hlkwd{message}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'saved'}\hlstd{,fname))}  \hlcom{# message to running R console}
   \hlkwd{print}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'saved'}\hlstd{,fname))}    \hlcom{# print to document}
\hlstd{\}} \hlkwa{else} \hlstd{\{}
   \hlkwd{message}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'skipped saving'}\hlstd{,fname))} \hlcom{# message to running R console}
   \hlkwd{print}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'skipped saving'}\hlstd{,fname))}   \hlcom{# print to document}
\hlstd{\}}
\end{alltt}
\begin{verbatim}
## [1] "skipped saving thRS500.Rdata"
\end{verbatim}
\begin{alltt}
\hlkwd{paste}\hlstd{(}\hlstr{'checked at'}\hlstd{,}\hlkwd{date}\hlstd{())}
\end{alltt}
\begin{verbatim}
## [1] "checked at Fri Nov  8 15:32:54 2013"
\end{verbatim}
\begin{alltt}
\hlkwd{system}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{'shasum'}\hlstd{,fname),}\hlkwc{intern}\hlstd{=T)}  \hlcom{# write down file hash}
\end{alltt}
\begin{verbatim}
## [1] "304895b8b5860ac5c995e10bd3b8c995820d60a0  thRS500.Rdata"
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{document}
